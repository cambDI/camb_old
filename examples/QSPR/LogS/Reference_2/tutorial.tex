\documentclass[twoside,a4wide,12pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
%\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em,frame=single}
%\DefineVerbatimEnvironment{Soutput}{Verbatim} {xleftmargin=2em,frame=single}
\usepackage[left=2.5cm,top=2cm,right=2cm,bottom=2.5cm,bindingoffset=0.5cm]{geometry}
\usepackage{amsmath} 
\usepackage[affil-it]{authblk}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{pdflscape}
\usepackage[backend=bibtex,sorting=none,style=ieee]{biblatex}
\usepackage{setspace}
\usepackage{inconsolata}
\bibliography{biblio}


\title{QSPR with 'camb'\\
{\bf C}hemically {\bf A}ware {\bf M}odel {\bf B}uilder\\
}

\author[1,5]{\rm Daniel S. Murrell\thanks{dsmurrell@gmail.com}}
\author[2,5]{\rm Isidro Cortes-Ciriano\thanks{isidrolauscher@gmail.com}} 
\author[3]{\rm Gerard J. P. van Westen}
\author[4]{\rm Ian P. Stott}
\author[1]{\rm Andreas Bender}
\author[2]{\rm Therese E. Malliavin}
\author[1]{\rm Robert C. Glen}
\affil[1]{Unilever Centre for Molecular Science Informatics, Department of Chemistry, University of Cambridge, Cambridge, United Kingdom.}
\affil[2]{Unite de Bioinformatique Structurale, Institut Pasteur and CNRS UMR 3825, Structural Biology and Chemistry Department, 25-28, rue Dr. Roux, 75 724 Paris, France.}
\affil[3]{ChEMBL Group, European Molecular Biology Laboratory European Bioinformatics Institute, Wellcome Trust Genome Campus, CB10 1SD, Hinxton, Cambridge, UK.}
\affil[4]{Unilever Research, Bebington, UK.}
\affil[5]{Equal contributors}
\setlength{\parindent}{0pt}

\setlength{\parskip}{\baselineskip}%
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\maketitle
\onehalfspacing



\maketitle

In the following sections, we demonstrate the utility of the \texttt{camb} \cite{camb} package by presenting a pipeline which generates various aqueous solubility models using 2D molecular descriptors calculated by the PaDEL-Descriptor package as input features. These models are then ensembled to create a single model with a greater predictive accuracy. The trained ensemble is then put to use in making predictions for new molecules.

Firstly, the package is loaded and the working directory set:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(camb)}
\hlcom{# setwd('path_to_working_directory')}
\end{alltt}
\end{kframe}
\end{knitrout}

\section{Compounds}

\subsection{Reading and Preprocessing}
The compounds are read in and standardised. Internally, Indigo's C API \cite{Indigo}, incorporated into the \texttt{camb} package, is use to perform this task.
Molecules are represented with implicit hydrogens, dearomatized, and passed through the InChI format to ensure that tautomers are represented by the same SMILES.

The \texttt{StandardiseMolecules} function allows representation of the molecular structures in a similarly processed form.
The arguments of this function allow control over the maximum number of (i) fluorines, (ii) chlorines,
(iii) bromines, and (iv) iodines the molecules can contain in order to be retained for training.
Inorgnaic molecules (those containing atoms not in \{H, C, N, O, P, S, F, Cl, Br, I\}) are removed if the argument \texttt{remove.inorganic} is set to \texttt{TRUE}. This is the function's default behaviour.
The upper and lower limits for molecular mass can be set with the arguments \texttt{min.mass.limit} and \texttt{max.mass.limit}.
The name of the file containing the chemical structures is provided by the argument \texttt{structures.file}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{std.options} \hlkwb{<-} \hlkwd{StandardiseMolecules}\hlstd{(}\hlkwc{structures.file}\hlstd{=}\hlstr{"solubility_2007_ref2.sdf"}\hlstd{,}
                                                \hlkwc{standardised.file}\hlstd{=}\hlstr{"standardised.sdf"}\hlstd{,}
                                                \hlkwc{removed.file}\hlstd{=}\hlstr{"removed.sdf"}\hlstd{,}
                                                \hlkwc{properties.file} \hlstd{=} \hlstr{"properties.csv"}\hlstd{,}
                                                \hlkwc{remove.inorganic}\hlstd{=}\hlnum{TRUE}\hlstd{,}
                                                \hlkwc{fluorine.limit}\hlstd{=}\hlnum{3}\hlstd{,}
                                                \hlkwc{chlorine.limit}\hlstd{=}\hlnum{3}\hlstd{,}
                                                \hlkwc{bromine.limit}\hlstd{=}\hlnum{3}\hlstd{,}
                                                \hlkwc{iodine.limit}\hlstd{=}\hlnum{3}\hlstd{,}
                                                \hlkwc{min.mass.limit}\hlstd{=}\hlnum{20}\hlstd{,}
                                                \hlkwc{max.mass.limit}\hlstd{=}\hlnum{900}\hlstd{)}
\hlkwd{saveRDS}\hlstd{(std.options,} \hlstr{"standardisation_options.rds"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
Molecules that Indigo manages to parse and that pass the filters are written to the file indicated by the \texttt{standardised.file} argument once they have been through the standardisation procedure. Molecules that were discarded are written to the file indicated by the \texttt{removed.file} argument. The molecule name and molecular properties specified in the structure file are written to the file indicated in the argument \texttt{properties.file} which is in CSV format. A column \texttt{kept} is added which indicates which molecules were deleted (0) or kept (1).

\section{Target Visualisation}
\texttt{camb} provides a function to visualise the density of the target variable. Visualising the distribution of the target variable gives can give a measure of how accurate a trained model is. The narrower the distribution the lower the RMSE should for the model to exhibit predictive power.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{properties} \hlkwb{<-} \hlkwd{read.table}\hlstd{(}\hlstr{"properties.csv"}\hlstd{,} \hlkwc{header}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{sep}\hlstd{=}\hlstr{"\textbackslash{}t"}\hlstd{)}
\hlstd{properties} \hlkwb{<-} \hlstd{properties[properties}\hlopt{$}\hlstd{Kept}\hlopt{==}\hlnum{1}\hlstd{, ]}
\hlkwd{head}\hlstd{(properties)}
\hlstd{targets} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{Name} \hlstd{= properties}\hlopt{$}\hlstd{NAME,} \hlkwc{target} \hlstd{= properties}\hlopt{$}\hlstd{EXPT)}
\hlstd{p} \hlkwb{<-} \hlkwd{DensityResponse}\hlstd{(targets}\hlopt{$}\hlstd{target)} \hlopt{+} \hlkwd{xlab}\hlstd{(}\hlstr{"LogS Target Distribution"}\hlstd{)}
\hlstd{p}
\end{alltt}
\end{kframe}\begin{figure}[]


{\centering \includegraphics[width=8cm]{figure/unnamed-chunk-5} 

}

\caption[LogS Target Distribution]{LogS Target Distribution\label{fig:unnamed-chunk-5}}
\end{figure}


\end{knitrout}

\subsection{PaDEL Descriptors}
One and two-dimensional descriptors can be calculated with the function \texttt{GeneratePadelDescriptors} provided by the PaDEL-Descriptor \cite{padel} Java library built into the \texttt{camb} package:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{descriptor.types} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"2D"}\hlstd{)}
\hlstd{descriptors} \hlkwb{<-} \hlkwd{GeneratePadelDescriptors}\hlstd{(}\hlkwc{standardised.file} \hlstd{=} \hlstr{"standardised.sdf"}\hlstd{,}
    \hlkwc{types} \hlstd{= descriptor.types,} \hlkwc{threads} \hlstd{=} \hlnum{1}\hlstd{)}
\hlstd{descriptors} \hlkwb{<-} \hlkwd{RemoveStandardisedPrefix}\hlstd{(descriptors)}
\hlkwd{saveRDS}\hlstd{(descriptors,} \hlkwc{file} \hlstd{=} \hlstr{"descriptors.rds"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}


\section{Statistical Pre-processing}
The descriptors and the target values are then merged by name into a single \textit{data.frame}. We check that the number of rows of the merged and original \textit{data.frames} are the same. We then split the \textit{data.frame} into \textit{ids}, \textit{x} and \textit{y} where \textit{ids} are the molecule names, \textit{x} is the block of descriptor values and \textit{y} is the target values.  
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{all} \hlkwb{<-} \hlkwd{merge}\hlstd{(}\hlkwc{x} \hlstd{= targets,} \hlkwc{y} \hlstd{= descriptors,} \hlkwc{by} \hlstd{=} \hlstr{"Name"}\hlstd{)}
\hlstd{ids} \hlkwb{<-} \hlstd{all}\hlopt{$}\hlstd{Name}
\hlstd{x} \hlkwb{<-} \hlstd{all[}\hlnum{3}\hlopt{:}\hlkwd{ncol}\hlstd{(all)]}
\hlstd{y} \hlkwb{<-} \hlstd{all}\hlopt{$}\hlstd{target}
\end{alltt}
\end{kframe}
\end{knitrout}

Sometimes, some descriptors are not calculated for all molecules, giving a "NA" or "Inf" as the descriptor value. 
Instead of removing that descriptor for all molecules, the missing descriptor values can be imputed from the corresponding descriptor values in the molecules that are closest to the molecule with the missing information.
"Inf" descriptor values are first converted to "NA".
For the imputation of missing descriptor values, the R package  \texttt{impute} is required.
Depending on the R version, it can be accessed from either \texttt{CRAN} or \texttt{Bioconductor}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: impute}}\begin{verbatim}
## Cluster size 1606 broken into 375 1231 
## Done cluster 375 
## Done cluster 1231
\end{verbatim}
\end{kframe}
\end{knitrout}

The dataset is randomly split into a training set (80\%) used for training and a holdout set (20\%) which used to assess the predictive ability of the models on molecules drawn from the same distribution as the training set. Unhelpful descriptos are removed: (i) those with a variance close to zero (near-zero variance), and (ii) those highly correlated with one another:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dataset} \hlkwb{<-} \hlkwd{SplitSet}\hlstd{(ids, x.imputed, y,} \hlkwc{percentage} \hlstd{=} \hlnum{20}\hlstd{)}
\hlstd{dataset} \hlkwb{<-} \hlkwd{RemoveNearZeroVarianceFeatures}\hlstd{(dataset,}
    \hlkwc{frequencyCutoff} \hlstd{=} \hlnum{30}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# 397 features removed with variance below cutoff}}\begin{alltt}
\hlstd{dataset} \hlkwb{<-} \hlkwd{RemoveHighlyCorrelatedFeatures}\hlstd{(dataset,}
    \hlkwc{correlationCutoff} \hlstd{=} \hlnum{0.95}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# 121 features removed with correlation above cutoff}}\end{kframe}
\end{knitrout}

The descriptors are converted to z-scores by centering them to have a mean of zero and scaling them to have unit variance:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dataset} \hlkwb{<-} \hlkwd{PreProcess}\hlstd{(dataset)}
\end{alltt}
\end{kframe}
\end{knitrout}

Five fold cross-validation (CV) is be used to optimize the hyperparameters of the models:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dataset} \hlkwb{<-} \hlkwd{GetCVTrainControl}\hlstd{(dataset,} \hlkwc{folds} \hlstd{=} \hlnum{5}\hlstd{)}
\hlkwd{saveRDS}\hlstd{(dataset,} \hlkwc{file} \hlstd{=} \hlstr{"dataset_logS_preprocessed.rds"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

All models are trained with the same CV options, {\it i.e.} the arguments of the function \texttt{GetCVTrainControl}, to allow ensemble modeling as ensemble modelling requires the same data fold split over all methods used.
It is important to mention that the functions presented in the previous code blocks depend on functions from the \texttt{caret} package, namely: 
\begin{itemize}
\item RemoveNearZeroVarianceFeatures : nearZeroVar
\item RemoveHighlyCorrelatedFeatures : findCorrelation
\item PreProcess : preProcess
\item GetCVTrainControl : trainControl
\end{itemize}
Experienced users might want to have more control over the underlying \texttt{caret} functions.
This is certainly possible as the arguments given to the \texttt{camb} functions will be subsequently given to the \texttt{caret} counterparts in the typical \texttt{...} R fashion.
In fact, experienced users may want to learn from the internals of the functions \texttt{camb} provides and create their own specialised pipeline that fits their own modelling needs. \texttt{camb} is intended to speed up the modelling process and should not limit use of the extremely valuble caret package which \texttt{camb} utilises.
The default values of these function permit the less experienced user to quickly handle the statistical preprocessing
steps with ease, making a reasonable default choice for the argument values.

\section{Model Training}

In the following section we present the different steps required to train a QSPR model with \texttt{camb}. It should be noted that the above steps can be run locally on a low powered computer such as a laptop and the preprocessed dataset saved to disk. This dataset can then be copied to a high powered machine or a farm with multiple cores for model training and the resulting models saved back to the local machine. Pro tip: Dropbox can be used to sync this proceedure so that manual transfer is not required.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dataset} \hlkwb{<-} \hlkwd{readRDS}\hlstd{(}\hlstr{"dataset_logS_preprocessed.rds"}\hlstd{)}
\hlcom{# register the number of cores to use in training}
\hlkwd{registerDoMC}\hlstd{(}\hlkwc{cores} \hlstd{=} \hlnum{10}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\subsection{Support Vector Machines (SVM)}
Firstly, a SVM using a radial basis function kernel is trained \cite{svmreview}.
An base 2 exponential grid is used to optimize over the hyperparameters. The \texttt{train} function from the \texttt{caret} package is used directly for model training.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(kernlab)}
\hlstd{method} \hlkwb{<-} \hlstr{"svmRadial"}
\hlstd{tune.grid} \hlkwb{<-} \hlkwd{expand.grid}\hlstd{(}\hlkwc{.sigma} \hlstd{=} \hlkwd{expGrid}\hlstd{(}\hlopt{-}\hlnum{8}\hlstd{,} \hlnum{4}\hlstd{,} \hlnum{2}\hlstd{,}
    \hlnum{2}\hlstd{),} \hlkwc{.C} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1e-04}\hlstd{,} \hlnum{0.001}\hlstd{,} \hlnum{0.01}\hlstd{,} \hlnum{0.1}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{10}\hlstd{,} \hlnum{100}\hlstd{))}
\hlstd{model} \hlkwb{<-} \hlkwd{train}\hlstd{(dataset}\hlopt{$}\hlstd{x.train, dataset}\hlopt{$}\hlstd{y.train, method,}
    \hlkwc{tuneGrid} \hlstd{= tune.grid,} \hlkwc{trControl} \hlstd{= dataset}\hlopt{$}\hlstd{trControl)}
\hlkwd{saveRDS}\hlstd{(model,} \hlkwc{file} \hlstd{=} \hlkwd{paste}\hlstd{(method,} \hlstr{".rds"}\hlstd{,} \hlkwc{sep} \hlstd{=} \hlstr{""}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}

\subsection{Random Forest}
We proceed similarly in the case of a random forest (RF) model \cite{rf}. Here, only the mtry parameter needs optimisation.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(randomForest)}
\hlstd{method} \hlkwb{<-} \hlstr{"rf"}
\hlstd{tune.grid} \hlkwb{<-} \hlkwd{expand.grid}\hlstd{(}\hlkwc{.mtry} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{5}\hlstd{,} \hlnum{100}\hlstd{,} \hlnum{5}\hlstd{))}
\hlstd{model} \hlkwb{<-} \hlkwd{train}\hlstd{(dataset}\hlopt{$}\hlstd{x.train, dataset}\hlopt{$}\hlstd{y.train, method,}
    \hlkwc{tuneGrid} \hlstd{= tune.grid,} \hlkwc{trControl} \hlstd{= dataset}\hlopt{$}\hlstd{trControl)}
\hlkwd{saveRDS}\hlstd{(model,} \hlkwc{file} \hlstd{=} \hlkwd{paste}\hlstd{(method,} \hlstr{".rds"}\hlstd{,} \hlkwc{sep} \hlstd{=} \hlstr{""}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}

\subsection{Gradient Boosting Machine}
A gradient boosting machine (GBM) model \cite{gbm} is trained optimising over the number of trees and the interaction depth.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(gbm)}
\hlstd{method} \hlkwb{<-} \hlstr{"gbm"}
\hlstd{tune.grid} \hlkwb{<-} \hlkwd{expand.grid}\hlstd{(}\hlkwc{.n.trees} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{500}\hlstd{,} \hlnum{1000}\hlstd{),} \hlkwc{.interaction.depth} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{25}\hlstd{),}
    \hlkwc{.shrinkage} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0.01}\hlstd{,} \hlnum{0.02}\hlstd{,} \hlnum{0.04}\hlstd{,} \hlnum{0.08}\hlstd{))}
\hlstd{model} \hlkwb{<-} \hlkwd{train}\hlstd{(dataset}\hlopt{$}\hlstd{x.train, dataset}\hlopt{$}\hlstd{y.train, method,}
    \hlkwc{tuneGrid} \hlstd{= tune.grid,} \hlkwc{trControl} \hlstd{= dataset}\hlopt{$}\hlstd{trControl)}
\hlkwd{saveRDS}\hlstd{(model,} \hlkwc{file} \hlstd{=} \hlkwd{paste}\hlstd{(method,} \hlstr{".rds"}\hlstd{,} \hlkwc{sep} \hlstd{=} \hlstr{""}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}

For each model we determine if our hyper-parameter search needs to be altered. In the following we focus on the RF model, though the same steps are also applied to the GBM and SVM models. If the hyper-parameters scanned lead you to what looks like a global minimum then you can stop scanning the space of hyper-parameters, otherwise you need to adjust the grid and retrain your model.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model} \hlkwb{<-} \hlkwd{readRDS}\hlstd{(}\hlstr{"rf.rds"}\hlstd{)}
\hlkwd{plot}\hlstd{(model,} \hlkwc{metric} \hlstd{=} \hlstr{"RMSE"}\hlstd{)}
\end{alltt}
\end{kframe}\begin{figure}[]


{\centering \includegraphics[width=8cm]{figure/unnamed-chunk-17} 

}

\caption[CV RMSE over the mtry hyperparameter for the RF]{CV RMSE over the mtry hyperparameter for the RF\label{fig:unnamed-chunk-17}}
\end{figure}


\end{knitrout}

\section{Model Evaluation}

Once the models are trained, the cross validated metrics for the optimised hyper-parameters become visible:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(}\hlkwd{RMSE_CV}\hlstd{(model,} \hlkwc{digits} \hlstd{=} \hlnum{3}\hlstd{))}
\end{alltt}
\begin{verbatim}
## [1] 0.624
\end{verbatim}
\begin{alltt}
\hlkwd{print}\hlstd{(}\hlkwd{Rsquared_CV}\hlstd{(model,} \hlkwc{digits} \hlstd{=} \hlnum{3}\hlstd{))}
\end{alltt}
\begin{verbatim}
## [1] 0.8895
\end{verbatim}
\end{kframe}
\end{knitrout}

On the basis of the soundness of the obtained models, assessed through the value of the cross-validated metrics, 
we proceed to predict the values for the external (hold-out) set:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{holdout.predictions} \hlkwb{<-} \hlkwd{as.vector}\hlstd{(}\hlkwd{predict}\hlstd{(model}\hlopt{$}\hlstd{finalModel,}
    \hlkwc{newdata} \hlstd{= dataset}\hlopt{$}\hlstd{x.holdout))}
\end{alltt}
\end{kframe}
\end{knitrout}

To visualize the correlation between predicted and observed values, we use the \texttt{CorrelationPlot} function:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{CorrelationPlot}\hlstd{(}\hlkwc{pred} \hlstd{= holdout.predictions,} \hlkwc{obs} \hlstd{= dataset}\hlopt{$}\hlstd{y.holdout,}
    \hlkwc{PointSize} \hlstd{=} \hlnum{3}\hlstd{,} \hlkwc{ColMargin} \hlstd{=} \hlstr{"blue"}\hlstd{,} \hlkwc{TitleSize} \hlstd{=} \hlnum{26}\hlstd{,}
    \hlkwc{XAxisSize} \hlstd{=} \hlnum{20}\hlstd{,} \hlkwc{YAxisSize} \hlstd{=} \hlnum{20}\hlstd{,} \hlkwc{TitleAxesSize} \hlstd{=} \hlnum{24}\hlstd{,}
    \hlkwc{margin} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{PointColor} \hlstd{=} \hlstr{"black"}\hlstd{,} \hlkwc{PointShape} \hlstd{=} \hlnum{16}\hlstd{,}
    \hlkwc{MarginWidth} \hlstd{=} \hlnum{1}\hlstd{,} \hlkwc{AngleLab} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{xlab} \hlstd{=} \hlstr{"Observed"}\hlstd{,}
    \hlkwc{ylab} \hlstd{=} \hlstr{"Predicted"}\hlstd{)}
\end{alltt}
\end{kframe}\begin{figure}[]


{\centering \includegraphics[width=8cm]{figure/unnamed-chunk-21} 

}

\caption[Observed vs Predicted]{Observed vs Predicted\label{fig:unnamed-chunk-21}}
\end{figure}


\end{knitrout}

\section{Ensemble Modeling}
In the following section, two ensemble modeling techniques are applied, namely greedy optimization and model stacking.
Further information about these methods can be found in ref \cite{caretEnsemble} and \cite{caruana}.

We append all the trained models to a list. The \texttt{sort} function shows the cross-validation RMSEs of the trained models in accending order. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{all.models} \hlkwb{<-} \hlkwd{list}\hlstd{()}
\hlstd{all.models[[}\hlkwd{length}\hlstd{(all.models)} \hlopt{+} \hlnum{1}\hlstd{]]} \hlkwb{<-} \hlkwd{readRDS}\hlstd{(}\hlstr{"gbm.rds"}\hlstd{)}
\hlstd{all.models[[}\hlkwd{length}\hlstd{(all.models)} \hlopt{+} \hlnum{1}\hlstd{]]} \hlkwb{<-} \hlkwd{readRDS}\hlstd{(}\hlstr{"svmRadial.rds"}\hlstd{)}
\hlstd{all.models[[}\hlkwd{length}\hlstd{(all.models)} \hlopt{+} \hlnum{1}\hlstd{]]} \hlkwb{<-} \hlkwd{readRDS}\hlstd{(}\hlstr{"rf.rds"}\hlstd{)}

\hlcom{# sort the models from lowest to highest RMSE}
\hlkwd{names}\hlstd{(all.models)} \hlkwb{<-} \hlkwd{sapply}\hlstd{(all.models,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{) x}\hlopt{$}\hlstd{method)}
\hlkwd{sort}\hlstd{(}\hlkwd{sapply}\hlstd{(all.models,} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{min}\hlstd{(}\hlkwd{as.vector}\hlstd{(}\hlkwd{na.omit}\hlstd{(x}\hlopt{$}\hlstd{results}\hlopt{$}\hlstd{RMSE)))))}
\end{alltt}
\begin{verbatim}
##       gbm        rf svmRadial 
##    0.5903    0.6239    0.6320
\end{verbatim}
\end{kframe}
\end{knitrout}
    
A greedy ensemble is then trained using 1000 iterations. The Greedy ensemble picks a linear combination of model outputs that is a local minimum in the RMSE landscape. The weights for each model can be seen in the \texttt{greedy\$weights} variable. The RMSE of the greedy model can be found in the \texttt{greedy\$error} variable.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{greedy} \hlkwb{<-} \hlkwd{caretEnsemble}\hlstd{(all.models,} \hlkwc{iter} \hlstd{=} \hlnum{1000}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: pbapply}}\begin{alltt}
\hlkwd{sort}\hlstd{(greedy}\hlopt{$}\hlstd{weights,} \hlkwc{decreasing} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       gbm svmRadial        rf 
##     0.580     0.336     0.084
\end{verbatim}
\begin{alltt}
\hlkwd{saveRDS}\hlstd{(greedy,} \hlkwc{file} \hlstd{=} \hlstr{"greedy.rds"}\hlstd{)}
\hlstd{greedy}\hlopt{$}\hlstd{error}
\end{alltt}
\begin{verbatim}
##   RMSE 
## 0.5742
\end{verbatim}
\end{kframe}
\end{knitrout}
 
Similarly, we create a linear stack ensemble that uses the trained model inputs as input into the stack.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{linear} \hlkwb{<-} \hlkwd{caretStack}\hlstd{(all.models,} \hlkwc{method} \hlstd{=} \hlstr{"glm"}\hlstd{,} \hlkwc{trControl} \hlstd{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{))}
\hlkwd{saveRDS}\hlstd{(linear,} \hlkwc{file} \hlstd{=} \hlstr{"linear.rds"}\hlstd{)}
\hlstd{linear}\hlopt{$}\hlstd{error}
\end{alltt}
\begin{verbatim}
##   parameter  RMSE Rsquared  RMSESD RsquaredSD
## 1      none 0.574   0.9035 0.04047    0.01881
\end{verbatim}
\end{kframe}
\end{knitrout}

We also create a non-linear stack ensemble that uses the trained model inputs as input into the stack. In this case we use a Random Forest as the stacking model.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{registerDoMC}\hlstd{(}\hlkwc{cores} \hlstd{=} \hlnum{1}\hlstd{)}
\hlstd{tune.grid} \hlkwb{<-} \hlkwd{expand.grid}\hlstd{(}\hlkwc{.mtry} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwd{length}\hlstd{(all.models),}
    \hlnum{1}\hlstd{))}
\hlstd{nonlinear} \hlkwb{<-} \hlkwd{caretStack}\hlstd{(all.models,} \hlkwc{method} \hlstd{=} \hlstr{"rf"}\hlstd{,}
    \hlkwc{trControl} \hlstd{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{),} \hlkwc{tune.grid} \hlstd{= tune.grid)}
\end{alltt}
\begin{verbatim}
## note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .
\end{verbatim}
\begin{alltt}
\hlkwd{saveRDS}\hlstd{(nonlinear,} \hlkwc{file} \hlstd{=} \hlstr{"nonlinear.rds"}\hlstd{)}
\hlstd{nonlinear}\hlopt{$}\hlstd{error}
\end{alltt}
\begin{verbatim}
##   mtry   RMSE Rsquared  RMSESD RsquaredSD
## 1    2 0.6107   0.8922 0.07509    0.02504
## 2    3 0.6195   0.8892 0.07569    0.02534
\end{verbatim}
\end{kframe}
\end{knitrout}

The greedy and the linear stack ensembles have a cross validated RMSEs that are lower than any of the individual models.

We then test to see if these ensemble models outperform the individual models on the holdout set.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{preds} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwd{sapply}\hlstd{(all.models, predict,} \hlkwc{newdata} \hlstd{= dataset}\hlopt{$}\hlstd{x.holdout))}
\hlstd{preds}\hlopt{$}\hlstd{ENS_greedy} \hlkwb{<-} \hlkwd{predict}\hlstd{(greedy,} \hlkwc{newdata} \hlstd{= dataset}\hlopt{$}\hlstd{x.holdout)}
\hlstd{preds}\hlopt{$}\hlstd{ENS_linear} \hlkwb{<-} \hlkwd{predict}\hlstd{(linear,} \hlkwc{newdata} \hlstd{= dataset}\hlopt{$}\hlstd{x.holdout)}
\hlstd{preds}\hlopt{$}\hlstd{ENS_nonlinear} \hlkwb{<-} \hlkwd{predict}\hlstd{(nonlinear,} \hlkwc{newdata} \hlstd{= dataset}\hlopt{$}\hlstd{x.holdout)}
\hlkwd{sort}\hlstd{(}\hlkwd{sqrt}\hlstd{(}\hlkwd{colMeans}\hlstd{((preds} \hlopt{-} \hlstd{dataset}\hlopt{$}\hlstd{y.holdout)}\hlopt{^}\hlnum{2}\hlstd{)))}
\end{alltt}
\begin{verbatim}
##    ENS_linear    ENS_greedy           gbm ENS_nonlinear 
##        0.5109        0.5126        0.5156        0.5515 
##            rf     svmRadial 
##        0.5938        0.5984
\end{verbatim}
\end{kframe}
\end{knitrout}

The linear ensemble slightly outperforms other models on the holdout set as well. This leads us to choose this as the most predictive model for future predictions. In the case that the ensemble models underperform the single models on the holdout set, it is advisable to pick the best single model for future predictions as a simpler model is a better model performance being equal. 

\section{External Predictions}

One of the main attractions of this package is that it makes standardising and making predictions on new molecules a simple task. It is essential to ensure that the same standardisation options and descriptor types are used when the model is applied to make predictions for new molecules.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{test_structures_file} \hlkwb{<-} \hlkwd{system.file}\hlstd{(}\hlstr{"test_structures"}\hlstd{,}
    \hlstr{"structures_10.sdf"}\hlstd{,} \hlkwc{package} \hlstd{=} \hlstr{"camb"}\hlstd{)}
\hlstd{predictions} \hlkwb{<-} \hlkwd{PredictExternal}\hlstd{(test_structures_file,}
    \hlstd{std.options, descriptor.types, dataset, linear)}
\end{alltt}
\begin{verbatim}
## [1] "Standardising Structures: Reading SDF (R)"
## [1] "Generating Descriptors"
\end{verbatim}
\begin{alltt}
\hlkwd{print}\hlstd{(predictions)}
\end{alltt}
\begin{verbatim}
##         id prediction
## 1  B000088    -1.9469
## 2  B000139    -2.1535
## 3  B000236    -6.3559
## 4  B000310    -4.9248
## 5  B000728    -3.2997
## 6  B000785    -2.4975
## 7  B000821    -2.1867
## 8  B000826    -3.1167
## 9  B001153     0.4922
## 10 B001156    -1.1043
\end{verbatim}
\end{kframe}
\end{knitrout}

\printbibliography

\end{document}
