\documentclass[twoside,a4wide,12pt]{article}
%\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em,frame=single}
%\DefineVerbatimEnvironment{Soutput}{Verbatim} {xleftmargin=2em,frame=single}
\usepackage[left=2.5cm,top=2cm,right=2cm,bottom=2.5cm,bindingoffset=0.5cm]{geometry}
\usepackage{amsmath} 
\usepackage[affil-it]{authblk}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{pdflscape}
\usepackage[backend=bibtex,sorting=none,style=ieee]{biblatex}
\usepackage{setspace}
\bibliography{biblio}


\title{Proteochemometrics (PCM) with 'camb'\\
{\bf C}hemistry {\bf A}ware {\bf M}odel {\bf B}uilder\\
}

\author[1,3]{\rm Isidro Cortes-Ciriano\thanks{isidrolauscher@gmail.com}} 
\author[2,3]{\rm Daniel Murrell\thanks{dsmurrell@gmail.com}}
\affil[1]{Unite de Bioinformatique Structurale, Institut Pasteur and CNRS UMR 3825, Structural Biology and Chemistry Department, 25-28, rue Dr. Roux, 75 724 Paris, France.}
\affil[2]{Unilever Centre for Molecular Science Informatics, Department of Chemistry, University of Cambridge, Cambridge, United Kingdom.}
\affil[3]{Equal contributors}
\setlength{\parindent}{0pt}


\begin{document}

\maketitle
\onehalfspacing

<<include=FALSE>>=
opts_chunk$set(concordance=TRUE)
opts_chunk$set(dev='pdf')
Sys.setenv(TEXINPUTS=getwd(),
           BIBINPUTS=getwd(),
           BSTINPUTS=getwd())
@

<<echo=FALSE,results='hide'>>=
options(width=60)
suppressWarnings(library(png,quietly=TRUE,warn.conflicts=FALSE))
suppressWarnings(library(caret,quietly=TRUE,warn.conflicts=FALSE))
suppressWarnings(library(doMC,quietly=TRUE,warn.conflicts=FALSE))
library(ggplot2,quietly=TRUE)
@
\maketitle

In the following sections, we present a pipeline to generate a Proteochemometric (PCM) model for mammal cyclooxigenase (COX) inhibitors. For futher details about PCM, the interested reader is referred to ref
\cite{review_pcm} and \cite{cortesReview}.
Firstly, we load the package and set the working directory:
<<echo=FALSE,results='hide',warning=FALSE,message=FALSE>>=
library(camb)
setwd('/Users/icortes/Desktop/camb_final/camb/examples/PCM/COX')
@
<<echo=TRUE,results='hide',warning=FALSE,message=FALSE>>=
library(camb)
#setwd('path_to_working_directory')
@

\section{Compounds}

\subsection{Reading and Preprocessing}
We proceed to read the compounds. Given that some smiles contain smarts patterns where the hash symbol is present, 
it is necessary to switch off the argument {\it comment.char} in order not to clip the smiles:
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
smiles <- read.table("smiles_COX.smi", header=FALSE,comment.char=c(""))
@

The function {\it StandardiseMolecules} enables the depiction of molecular structures in a similar way.
Moreover, the different arguments allow to control the maximum number of (i) fluorines, (ii) chlorines,
(iii) bromines, and (iv) iodines a molecules can exhibit in order to be kept. If using the default value, {\it i.e.} "-1",
all molecules will be kept irrespective of the number of fluorines, chlorines, bromines, and iodines.
Inorgnaic molecules are removed if the argument "removed.inorganic" is set to "TRUE", which is the default value.
Additionally, upper and lower limits for the molecular mass can be set with the arguments "min.mass.limit" and "max.mass.limit".
The default value is "-1". Thus, all molecules will be kept irrespective of their molecular mass.
The name of the file containing the chemical structures is input to the argument "structures.file".
<<echo=TRUE,results='hide',warning=FALSE,message=FALSE,eval=FALSE>>=
StandardiseMolecules(structures.file="smiles_COX.smi",
standardised.file="standardised.sdf",
removed.file="removed.sdf",
output="standardisation_COX_info.csv",
remove.inorganic=TRUE,
fluorine.limit=-1,
chlorine.limit=-1,
bromine.limit=-1,
iodine.limit=-1,
min.mass.limit=-1, #suggested value 20
max.mass.limit=-1) #suggested value  900
@
The properties of all molecules and an index, in the column "kept", indicating which molecules were deleted,
are written to the file indicated in the argument "output". In this case the file is \verb|"standardisation_COX_info.csv"|.
Those molecules that passed the filters are written to the file indicated in the argument "standardised.file".
By contrast, molecules that did not pass these filters are written to the file indicated in the "removed.file" argument.

<<echo=TRUE,results='hide',warning=FALSE,message=FALSE,eval=FALSE>>=
standardised_info <- read.table("standardisation_COX_info.csv",header=TRUE,sep="\t")
@

Default values of the arguments of the function "StandardiseMolecules" are not stringent.
In the present case, all molecules were kept, thus all values in the columns "kept"
are equal to "1".

The properties of a ".sdf" file can be inspected witn the function:
<<echo=TRUE,results='hide',warning=FALSE,message=FALSE,eval=FALSE>>=
ShowPropertiesSDF("standardised.sdf")
@
The values corresponding to an individual property can be accessed with the function "GetPropertySDF".
Similarly, the function "GetPropertiesSDF" retrieves the information for all properties of a given ".sdf" file. A data.frame with all properties is returned.
The number of molecules from which the information has to be retrieved can be indicated with the argument \verb|"number_processed"|.
The default value for this argument is \verb|"-1"|, which indicates that the properties will be extracted for all molecules in the input file.
<<echo=TRUE,results='hide',warning=FALSE,message=FALSE,eval=TRUE>>=
GetPropertySDF("standardised.sdf",property="property_name",number_processed=10)
all_properties <- GetPropertiesSDF("standardised.sdf",number_processed=10)
head(all_properties)
@

\subsection{PaDEL Descriptors}
One and two-dimensional PaDEL\cite{padel} descriptors and fingerprints can be calculated with the function "GeneratePadelDescriptors":
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
descriptors_COX <- GeneratePadelDescriptors( +
  standardised.file="smiles_COX.smi",threads = 1)
descriptors <- RemoveStandardisedPrefix(descriptors)
#saveRDS(descriptors, file="Padel_COX.rds")
#descriptors <- readRDS("Padel_COX.rds")
@

Sometimes, some descriptors are not calculated for all molecules, thus giving a "NA" or "Inf" as descriptor value.
Instead of removing that descriptor for all molecules, the missing descriptor values can be imputed from the corresponding descriptor values of the rest of molecules.
Descriptor values equal to "Inf" are converted to "NA".
For the imputation of missing dscriptor values, the R package {\it impute} is required.
Depending on the R version, it can be accessed from either {\it CRAN} or {\it Bioconductor}.

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),eval=FALSE>>=
descriptors <- ReplaceInfinitesWithNA(descriptors)
descriptors <- ImputeFeatures(descriptors)
@

\subsection{Circular Morgan Fingerprints}
The calculation of circular Morgan fingerprints requires the python library RDkit, given that the function "MorganFPs" calls a python script for the calculation of this type of fingerprints.
For a detailed discussion about circular Morgan fingerprints, we refer
the interested reader to ref. \cite{morgan}.
When using integrated development environments (IDE) such as RStudio,
the environment variables might not be defined within the R session.
However, they can be redefined with the R function "Sys.setenv".
In any case, the function "MorganFPs" requires this information in the arguments "PythonPath", path to python in the system, and "RDkitPath", the path to the RDkit library. 
For instance, the information of the latter is contained in the environment variable \verb|$RDBASE| in Mac OS.

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),eval=FALSE>>=
Sys.setenv(RDBASE="/usr/local/share/RDKit")
Sys.setenv(PYTHONPATH="/usr/local/lib/python2.7/site-packages")
fps_COX_512 <- MorganFPs(bits=512,radius=2,type='smi',mols='smiles_COX.smi', 
                         output='COX',keep='hashed_counts',
                         RDkitPath='/usr/local/share/RDKit',
                         PythonPath='/usr/local/lib/python2.7/site-packages',
                         verbose=TRUE, images = FALSE, unhashed = FALSE, 
                         extFileExtension = FALSE, extMols = FALSE, unhashedExt = FALSE, 
                         logFile = FALSE)
saveRDS(fps_COX_512,file="fps_COX_512.rds")
fps_COX_512 <- readRDS("fps_COX_512.rds")
@

The function 'MorganFPs" enables the calculation of the following types of fingerprints:
\begin{itemize}

\item Hashed fingerprints in {\bf binary format} of a given number of bits (argument 'bits') considering substructures with a maximum radius, defined with the argument 'radius' for the molecules specified in the argument 'mols'. These fingerprints are dropped to the output file \verb|COX_hashed_binary.csv|, where output corresponds to the value of the argument 'output'. In the example above: 'COX'. 
In hashed fingerprint several substructures can be mapped to the same bit position.
Therefore, it might be important for the sake of interpretability to know which substructures are mapped to which bit in the fingerprint. This information is given in the file \verb|COX_features_per_bit_hashed_fp.csv|.
The first column of the file corresponds to the bit index, whereas the remaining columns correspond to substructure ids.
This file is created automatically every time the function is run.

\item Hashed fingerprints in {\bf counts format} of a given number of bits (argument 'bits') considering substructures with a maximum radius, defined with the argument 'radius' for the molecules specified in the argument 'mols'. These fingerprints are dropped to the output file \verb|COX_hashed_counts.csv|.

\item Unhashed fingerprints in {\bf binary format}. All substructures present in the input molecules (argument 'mols') will be considered. Each position in the unhashed fingerprint corresponds to a given substructures. Thus, the resulting fingeprints are keyed. The smiles for each substructure and the number of atoms thereof is given in the output file \verb|COX_smiles_substructures.csv|. This file is created automatically every time the function is run. 
Unhashed fingerprints in {\bf binary format} are dropped to the output file \verb|COX_unhashed_binary.csv|.

\item Unhashed fingerprints in {\bf counts format}. All substructures present in the input molecules (argument 'mols') will be considered. Each position in the unhashed fingerprint corresponds to a given substructures. Thus, the resulting fingeprints are keyed. In contrast to binary format, where a given bit is set on if a substructure appears in a molecule irrespective of the number of times the substructure is present therein, the number of occurrences of each substructure is accounted. The smiles for each substructure and the number of atoms thereof is given in the output file \verb|COX_smiles_substructures.csv|. These fingerprints are dropped to the output file \verb|COX_unhashed_counts.csv|.

\end{itemize}

The index of the molecules that could not be handled during the calculation are dropped to the file \verb|incorrect_molecules_COX.csv|.

In the following paragraph we describe in detail the arguments of the function "MorganFPs":
\begin{itemize}
\item {\bf bits:} Number of bits of the hashed fingerprints. The default value is 512.
\item {\bf radius:} Maximum radius of the substructures. A radius of 2 is equivalent to \verb|ECFP-4|, where 4 corresponds to the diameter. The default value is 2.
More information on ECFP fingerprints can be found here: \verb|http://www.chemaxon.com/jchem/doc/user/ECFP.html|.
\item {\bf type:} File format containing the input molecules. The default value is ".smi".
\item {\bf mols:} File containing the input molecules.
\item {\bf output:} Label that will be appended to all ouput files (see below).
\item {\bf keep:} The fingeprints that will be kept after the calculation.
Apart from calculating different types of fingerprints, the function returns a data.frame with the type of fingerprints specified with this argument.  Possible types are: \verb|hashed_binary|,\verb|hashed_counts|, \verb|unhashed_binary|, \verb|unhashed_counts|, and if applicable, \verb|hashed_binaryEXT|, \verb|hashed_countsEXT|, \verb|unhashed_binaryEXT| and \verb|unhashed_countsEXT|.
The default value is \verb|"hashed_binary"|.
\item {\bf images:} If TRUE, individual ".pdf" files containing (i) the image of each substructure in the context of a molecule presenting it, and (ii) each molecule correctly processed, are created.  Be aware that the number of substructures can be large depending on the number and diversity of the molecules present in the input file. Thus, allow for sufficient memory in those cases.
The default value is FALSE.
\item {\bf unhashed:} If TRUE, unhashed fingeprints, both in binary format and with counts, are calculated. The default value is FALSE.
\item {\bf verbose:} If TRUE, information about the progression of the calculation is printed. The default value is FALSE.
\item {\bf RDkitPath:} The path to the folder containing the RDkit library in your computer. On mac, the environment variable \verb|$RDBASE| contains this information. The default value is "/usr/local/share/RDKit".
\item {\bf PythonPath:} Path to python (\verb|$PYTHONPATH|). The default value is "/usr/local/lib/python2.7/site-packages".
\item {\bf extFileExtension:} If not FALSE, file extension for the file containing the molecules for which unhashed fingeprints are to be calculated with respect to the pool of substructures in the molecules present in the file specified in "mols". The default value is FALSE.
\item {\bf extMols:} If not FALSE, file containing the molecules for which unhashed fingeprints are to be calculated with respect to the pool of
substructures in the molecules present in the file specified in "mols". The default value is FALSE.
\item {\bf unhashedExt:} If TRUE, unhashed fingerprints are calcualted for the molecules specified in "extMols". The default value is FALSE.
\item {\bf logFile:} If not FALSE, file where the log messages will be dropped. The default value is FALSE.
\end{itemize}


add all options of the functions
%Say the files that are generated.
%"_smiles_substructures.csv"

\section{Targets}

\subsection{Read and Preprocessing}
We read the amino acids from a .csv file:
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),eval=FALSE,tidy=TRUE>>=
amino_compound_IDs <- read.table("AAs_COX.csv",sep=",",
                                 header=TRUE,colClasses=c("character"),
                                 row.names=1)
amino_compound_IDs <- amino_compound_IDs[,2:ncol(amino_compound_IDs)]
@
Now, 5 Z-scales are calculated, which will serve to describe the target space in the PCM models.
Subsequently, we save the descriptors to a ".rds" file.
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),eval=FALSE,tidy=TRUE>>=
amino_compound_IDs_zscales <- AA_descs(Data=amino_compound_IDs,type="Z5")
saveRDS(amino_compound_IDs_zscales,file="Z5_COX.rds")
@

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=TRUE>>=
amino_compound_IDs_zscales <-readRDS("Z5_COX.rds")
@

In the case that we needed whole sequence descriptors, they can be calculated with the function "SeqDescs". 
The function takes as argument either a UniProt identifier, or either a matrix or dataframe with the protein sequences.
If a UniProt identifier is provided, the function gets firstly the sequence and then calculates the descriptors on the sequence.

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
Seq_descriptors_P00374 <- SeqDescs("P00374",UniProtID=TRUE,type=c("AAC","DC"))
@
The available types of whole sequence descriptors are:\cite{protr}
\begin{itemize}
\item Amino Acid Composition ("AAC")
\item Dipeptide Composition ("DC")
\item Tripeptide Composition ("TC")
\item Normalized Moreau-Broto Autocorrelation ("MoreauBroto")
\item Moran Autocorrelation ("Moran")
\item Geary Autocorrelation ("Geary")
\item  CTD (Composition/Transition/Distribution) ("CTD")
\item Conjoint Traid ("CTriad")
\item Sequence Order Coupling Number ("SOCN")
\item Quasi-sequence Order Descriptors ("QSO")
\item Pseudo Amino Acid Composition ("PACC")
\item Amphiphilic Pseudo Amino Acid Composition ("APAAC")
\end{itemize}

\subsection{Reading the Data-set Information}
Now, we are going to read the file with the information about the dataset, namely: target names, bioctivities, etc..
Note that when reading smiles from a ".csv" file into an R dataframe, the smiles are clipped after a hash ("\#") symbol.
A good practice is thus to also keep the smiles alone in a \{.smi,.smiles\} file.

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=TRUE>>=
dataset <- readRDS("COX_dataset_info.rds")
bioactivity <- dataset$standard_value
@
The bioactivity is in nM. We convert it to pIC50. To do that, we multiply by \verb|10^-9| to convert the bioactivity units to M.
Subsequently, the negative logarithm to base 10 is calculated:
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE>>=
bioactivity <- bioactivity * 10^-9
bioactivity <- -log(bioactivity,base=10)
@

\section{Data-set Visualization}
Compounds can be depicted with the function "PlotMolecules".
This function returns a list of four plots. Additionally, plots can also be written into a ".pdf" file if the argument "pdf.file" is not NULL. The argument "IDs" corresponds to the index of the molecules in the input file which are to be depicted.
The name of the molecule in the input file will be used as the title of the image if the argument "useNameAsTitle" is set to TRUE.

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=TRUE,results='hide',echo=FALSE>>=
# we redefine the function isnot
is.intalled <- function(mypkg) is.element(mypkg, installed.packages()[,1])
@

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,fig.align='center',fig.cap="Example of compoud depiction.",out.width='12cm',eval=TRUE,echo=TRUE,message=FALSE>>=
plot_molecules <- PlotMolecules("standardised.sdf",IDs=c(1,2,3,4),pdf.file=NULL,useNameAsTitle=TRUE,PDFMain=NULL) 
print(plot_molecules[[1]])
@

The distribution of the response variable can be explored with the function "DensityResponse" in the following way:  variable:
<<highlight=TRUE,results='hide',tidy.opts=list(width.cutoff=50),tidy=TRUE,fig.align='center',fig.cap="Bioactivity Distribution",out.width='12cm',eval=FALSE>>=
dens_resp <- DensityResponse(bioactivity,xlab="pIC50",main="",ylab="Densitiy",TitleSize=30,XAxisSize=22,YAxisSize=22,TitleAxesSize=24,AngleLab=0,lmar=0,rmar=0,bmar=0,tmar=0,binwidth=0.3)
@
%##
Plotting a PCA analysis of the target descriptors gives:
<<highlight=TRUE,results='hide',tidy.opts=list(width.cutoff=50),tidy=TRUE,fig.align='center',fig.cap="PCA Analysis on the\nAmino Acid Descriptors",eval=TRUE>>=
target_PCA <- PCAProt(amino_compound_IDs_zscales,
                      SeqsName=dataset$accession)
plot_PCA_Cox <- PCAProtPlot(target_PCA,PointSize=10,main="",TitleSize=30,XAxisSize=20,YAxisSize=20,TitleAxesSize=28,LegendPosition="bottom",RowLegend=3,ColLegend=5,LegendTitleSize=15,LegendTextSize=15)
@

incluir en PCAprotta
aa = summary(ana)
aa$importance

aa$sdev

incluir tanimoto


Similarly, we can analyze the chemical space by calculating pairwise compound similarities based upon the compound descriptors. In this case, we use the Jaccard metric to calculate the distance between compounds.
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
pw_dist_comp_fps <- PairwiseDist(fps_COX_512,method="jaccard")
saveRDS(pw_dist_comp_fps,file="pairwise_dist_COX.rds")
@
% XXX
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
pw_dist_comp_fps <- readRDS("pairwise_dist_COX.rds")
plot_pwd <- PairwiseDistPlot(pw_dist_comp_fps,xlab="Jaccard Similarity",ylab="Density",TitleSize=26,XAxisSize=20,YAxisSize=20,TitleAxesSize=24,lmar=0,rmar=0,bmar=0,tmar=0,AngleLab=0)
@

<<highlight=TRUE,tidy=TRUE,fig.align='center',fig.cap="Density of the response variable (upper panel). Pairwise Compound Jaccard Similarity (bottom pannel)",fig.width='10cm',fig.height='25cm'>>=
grid.arrange(dens_resp,plot_pwd,nrow=2)
@

%\newpage
Before any modeling attempt, it is interesting to know which is the maximum performance achievable {\it on the basis} of the available data.\\ 
By that, we consider the experimental uncertainty and the size of our data-set. 
In this case, a Gaussian Process (GP) model was trained in Matlab (data not shown) where the experimental uncertainty was optimized as a hyperparameter. The obtained value was 0.60.\\
This value is in accordance with recently published value of 0.68 for public IC50 data.
With the function 'MaxPerf', we can calculate the maximum achievable performance:

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
max_performance <- MaxPerf(meanNoise=0,sdNoise=0.6,meanResp=mean(bioactivity),sdResp=sd(bioactivity),lenPred=800)
@

The function returns a list of four plots. By using the function 'plotGrid' we can create a grid of plots in the following way:

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
plotGrid(plots=c(max_performance$p1,max_performance$p2,max_performance$p3,max_performance$p4))
@

\section{Statistical Pre-processing}
Bioactivity annotations in ChEMBL are sometimes redundant, meaning that for a given target-compound combination there are more than one annotated values.\\
To avoid this issue, we will remove redundant pairs and will keep the mean bioactivity value for those compound-target combinations repeated.\\
%To do that, we run the file \"remove_duplicates.R\":
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
source("remove_duplicates.R")
@

Now, we load the dataset without repetitions generated in the previous step. In addition, we remove those columns not containing descriptors (e.g. compound name):
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
dataset <- readRDS("Whole_dataset_NO_REP.rds")
killset <- expression(c(tid,pref_name,accession,organism,chembl_id,standard_value,standard_units, standard_type,chembl_id.1,Name,Name.1,Name.2,rows))
bioactivity <- dataset$standard_value
compound_IDs <- dataset$chembl_id.1
dataset <- subset(dataset,select=-eval(killset))
@

Subsequently, we split the dataset into training (70\%) and hold-out (external; 30\%) sets that will be used to assess the predictive ability of the models. Furthermore, we remove the following descriptors: (i) those with a variance close to zero (near-zero variance), and (ii) those highly correlated:
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
# Split the dataset into a training and holdout set
dataset <- SplitSet(compound_IDs, dataset, bioactivity, percentage=30)

# Remove the descriptors that are highly correlated or have low variance
dataset <- RemoveNearZeroVarianceFeatures(dataset,frequencyCutoff=30/1)
dataset <- RemoveHighlyCorrelatedFeatures(dataset)
@

We convert the descriptors to z-scores by centering them to zero mean and scaling their values to unit variance:
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
dataset <- PreProcess(dataset)
@

Given that cross-validation (CV) will be used to optimize the hyperparameters of the models, we divide the training set in 5 folds:
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
dataset <- GetCVTrainControl(dataset,seed=1,folds=5,repeats=1,
                             returnResamp='none',returnData=FALSE,
                             savePredictions=TRUE,verboseIter=TRUE, 
                             allowParallel=TRUE,
                             index=createMultiFolds(y.train, k=folds, times=repeats))
saveRDS(dataset, file="dataset_COX_preprocessed.rda")
@
All models are trained with the same CV options, {\it i.e.} hte arguments of the function 'GetCVTrainControl' to allow ensemble modeling (see below).


\section{Model Training}

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE>>=
dataset <- readRDS("dataset_COX_preprocessed.rda")
# Number of cores to be used during model training
cores <- 3
registerDoMC(cores) 
@

\subsection{Support Vector Machines (SVM)}
Firstly, a SVM will be trained\cite{svmreview}. We define an exponential grid (base 2) to optimize the hyperparameters:

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
method <- "svmRadial"
exp_grid <- expGrid(power.from=-8,power.to=-6,power.by=2,base=2)
tune.grid <- expand.grid(.sigma = exp_grid)
@

Training:
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
modelCoxSVMrad <- train(dataset$x.train, dataset$y.train, method, tuneGrid=tune.grid, trControl=dataset$trControl)
saveRDS(modelCoxSVMrad, file="model_SVM.rds")
@

\subsection{Random Forest}
We proceed similarly in the case of a random forest (RF) model\cite{rf}.

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
method <- "rf"

modelCoxRF<- train(dataset$x.train, dataset$y.train, method, trControl=dataset$trControl)
saveRDS(modelCoxRF, file="model_RF.rds")
@
Loading the RF model.
<<highlight=FALSE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=TRUE>>=
modelCoxRF<- readRDS("model_RF.rds")
@

\subsection{Gradient Boosting Machine}
We proceed similarly in the case of a gradient boosting machine (GBM) model\cite{gbm}.

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
method <- "gbm"
tune.grid <- expand.grid(.shrinkage=c(0.04, 0.08, 0.12, 0.16), 
                         .n.trees=c(500), .interaction.depth=c(25))   
modelCoxGBM<- train(dataset$x.train, dataset$y.train, method, tuneGrid=tune.grid, trControl=dataset$trControl)
saveRDS(modelCoxGBM, file="model_GBM.rds")
@


\section{Model Evaluation}

Once the models are trained, the cross validated metrics can be calculated:
We assume the metric used for the choice of the best combination of hyperparameters is 'RMSE'.
In the following we focus on the RF model, though the same can be applied to the GBM and SVM models.

<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE>>=
RMSE_CV_rf = signif(min(as.vector(na.omit(modelCoxRF$results$RMSE))), 
                    digits=3)
Rsquared_CV_rf = modelCoxRF$results$Rsquared[which(modelCoxRF$results$RMSE %in% min(modelCoxRF$results$RMSE, na.rm=TRUE))]
print(RMSE_CV_rf)
print(Rsquared_CV_rf)
@

On the basis of the soundness of the obtained models, we predict the values for the external (hold-out) set:
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,warning=FALSE,message=FALSE,eval=FALSE>>=
holdout.predictions <- as.vector(predict(modelCoxRF$finalModel, newdata = dataset$x.holdout))
@

We evaluate the predictive ability of our models by calculation the following statistical metrics:\\

{\bf Internal validation:}
\\
\begin{equation}
q_{{\it int}}^{2} = 1 - \frac {\sum_{i=1}^{N} (y_{i} - \widetilde{y}_{i})^{2}} {\sum_{i=1}^{N} (y_{i} - \bar{y}_{tr})^{2}}
\end{equation}
% cross-validated correlation coefficient

\begin{equation}
RMSE_{int} = \frac {\sqrt {(y_i - \widetilde{y}_i)^{2}}} {N}
\end{equation}

where $N$, $y_i$, $\widetilde{y}_i$ and $\bar{y}_{tr}$ represent the size of the training set, the observed, the predicted and the averaged values of the response variable for those datapoints included in the training set. The {\it i}th position within the training set is defined by {\it i}.  
\\
\\
{\bf External validation:}
\\

\begin{equation}
q_{{\it ext}}^{2} = 1 - \frac {\sum_{j=1}^{N} (y_j-\widetilde{y}_j)^{2}}  {\sum_{j=1}^{N} (y_j - \bar{y}_{ext})^{2}}
\end{equation}

\begin{equation}
RMSE_{ext} = \frac {\sqrt {(y_i - \widetilde{y}_i)^{2}}} {N} 
\end{equation}

\begin{equation}
R_{ext}^{2} = \frac {{\sum_{i=1}^{N} (y_{i} - \bar{y}_{ext})}  (\widetilde{y}_{i} - \overset{-}{\widetilde{y}_{ext}})} 
{\sqrt{\sum_{i=1}^{N} (y_{i} - \bar{y}_{ext})^{2} \sum{ (\widetilde{y}_{i} - \overset{-}{\widetilde{y}_{ext}})^{2}}}}
\end{equation}

\begin{equation}
R_{0\:ext}^2 = 1 - \frac {\sum_{j=1}^{N} (y_{j} - \widetilde{y}_{j}^{ r0})^{2}} {\sum_{j=1}^{N} (y_{j} - \bar{y}_{ext})^{2}} 
\end{equation}

where $N$, $y_j$, $\widetilde{y}_j$, $\bar{y}_{ext}$ and $\breve{y}_j$ represent the size of the training set, the observed, the predicted, and the averaged values of the response variable for those datapoints comprising the external set. The {\it j}th position within the external set is defined by {\it j}. $R_{0\:ext}^2$ is the square of the coefficient of determination through the origin, being $\widetilde{y}_{j}^{ r0} = k \widetilde{y}_j$ the regression through the origin (observed versus predicted) and $k$ its slope.\\
For a detailed discussion of both the evaluation of the predictive ability through the external set and different formulations for $q^{2}$, see ref.\cite{consonni}. 
To be considered as predictive, a model must satisfy the following criteria:\cite{beware,earnest}
\\
\begin{enumerate}
\item $q_{{\it int}}^{2} > 0.5$
\item $R_{ext}^2 > 0.6$
\item $ \frac {(R_{ext}^2 - R_{0\:ext}^2)} {R_{ext}^2} < 0.1$
\item $0.85 \leq k \leq 1.15$
\end{enumerate}

The metrics for the external validation are given by:
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
MetricsRf <- Validation(holdout.predictions,dataset$y.holdout)
@

To have a look at the correlation between predicted and observed values, we can use the 'CorrelationPlot' function:
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=TRUE,results='hide',echo=FALSE>>=
# loading the z.test values...
holdout.predictions <- readRDS("/Users/icortes/Desktop/COX/ztest_doc.rds") 
dataset$y.holdout <- readRDS("/Users/icortes/Desktop/COX/ytest_doc.rds")
@


<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,fig.align='center',fig.cap="Observed vs Predicted">>=
CorrelationPlot(pred=holdout.predictions,obs=dataset$y.holdout,PointSize=3,ColMargin='blue',TitleSize=26,XAxisSize=20,YAxisSize=20,TitleAxesSize=24,margin=2,PointColor="black",PointShape=16,MarginWidth=1,AngleLab=0,xlab="Observed",ylab="Predicted")
@

\section{Ensemble Modeling}
In the following section, we applied two ensemble modeling techniques, namely greedy optimization and model stacking, to create ensembles of models.
Further information can be found in ref \cite{caretEnsemble} and \cite{caruana}.
<<highlight=FALSE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=TRUE,echo=FALSE,message=FALSE,results='hide'>>=
suppressWarnings(require(devtools,quietly=T,warn.conflicts=FALSE)) || stop("package devtools need to be installed")
suppressWarnings(require(pbapply,quietly=TRUE,warn.conflicts=FALSE)) || stop("package pbapply need to be installed")
@

To get (i) the training set, (ii) the external set, (iii) the transformation applied to center and scale the descriptors before model training, and (iv) the model training options, we run the following lines:
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
data <- list()
attach(dataset)
data$transformation <- transformation
data$x.train <- x.train
data$y.train <- y.train
data$x.test <- x.holdout # external set
data$y.test <- y.holdout # external set
data$trControl <- trControl
saveRDS(data, file="data_ensemble.rds")
detach(dataset)
@
Subsequently, we load the models previously trained.
The list of models is in the file modelsEnsemble.
Once all models have been loaded, we create the following ensemble:
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
greedy <- caretEnsemble(all.models, iter=1000L)
sort(greedy$weights, decreasing=TRUE)

all.models <- list()
models <- as.vector(read.table("modelsEnsemble")$V1)

for (i in 1:length(models)){
  model_load = paste("readRDS('",models[i],"')",sep="")
  assign(paste("model_",i,sep=""), eval(parse(text=model_load)))
  all.models[[length(all.models)+1]] <- eval(parse(text=paste("model_",i,sep="")))
}

names(all.models) <- sapply(all.models, function(x) x$method)
sort(sapply(all.models, function(x) min(as.vector(na.omit(x$results$RMSE)))))
@

Once all models have been loaded, we create the following ensemble:
  %\begin{enumerate}
%\item Greedy (now it can only optimize the hold-out RMSE)
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
  greedy <- caretEnsemble(all.models, iter=1000L)
sort(greedy$weights, decreasing=TRUE)
@

%\item Model Stacking
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
  # make a linear regression ensemble
  linear <- caretStack(all.models, method='glm')
summary(linear$ens_model$finalModel)

# make Elastic Net ensemble
enet_ens <- caretStack(all.models, method='enet')
coefs_enet_ens <- enet_ens$ens_model$finalModel$beta.pure +
[ncol(enet_ens$ens_model$finalModel$beta.pure)+1,]

# make SVM linear ensemble
trControl <- trainControl(method = "cv",  number=5)
tune.grid <- expand.grid(.C=expGrid(power.from=-14,
                                    power.to=10,power.by=1,base=2))
linear_svm <- caretStack(all.models, method='svmLinear',
                         trControl=trControl,tuneGrid=tune.grid)

# make SVM radial ensemble
trControl <- trainControl(method = "cv",  number=5)
tune.grid <- expand.grid(.sigma=expGrid(power.from=-14,
                                        power.to=10,power.by=1,base=2),
                         .C=expGrid(power.from=-14,power.to=10,
                                    power.by=2,base=2))
radial_svm <- caretStack(all.models, method='svmRadial',
                         trControl=trControl,tuneGrid=tune.grid)
@
%\end{enumerate}
We proceed to predict the bioactivities for the external (hold-out) set,
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
preds <- data.frame(sapply(all.models, predict, newdata=dataa$x.test))
preds$ENS_greedy <- predict(greedy, newdata=dataa$x.test)
preds$ENS_linear <- predict(linear, newdata=dataa$x.test)
preds$ENS_enet <- predict(enet_ens, newdata=x.test)
preds$ENS_SVMrad <- predict(radial_svm, newdata=x.test)c
preds$ENS_SVMlin <- predict(linear_svm, newdata=x.test)
@

and we calculate the metrics:
  
<<highlight=TRUE,tidy.opts=list(width.cutoff=50),tidy=TRUE,eval=FALSE>>=
# Calculate metrics (We could also have applied Validation instead.)
Q2s <- apply(preds,2, function(x) Qsquared(x,dataa$y.test))
R2s <- apply(preds,2, function(x) Rsquared(x,dataa$y.test))
R20s <- apply(preds,2, function(x) Rsquared0(x,dataa$y.test))
RMSEs <- apply(preds,2, function(x) RMSE(x,dataa$y.test))
@
%\section{Bibliography}
\newpage
\printbibliography

\end{document}
